<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[My Blog]]></title>
  <link href="http://xuhappy.github.io/atom.xml" rel="self"/>
  <link href="http://xuhappy.github.io/"/>
  <updated>2021-09-02T11:53:22+08:00</updated>
  <id>http://xuhappy.github.io/</id>
  <author>
    <name><![CDATA[XU, Huanle]]></name>
    <email><![CDATA[xhlcuhk@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Talk something about my research]]></title>
    <link href="http://xuhappy.github.io/blog/2015/01/17/talk-something-about-my-research/"/>
    <updated>2015-01-17T00:38:32+08:00</updated>
    <id>http://xuhappy.github.io/blog/2015/01/17/talk-something-about-my-research</id>
    <content type="html"><![CDATA[<p>I am a postgraduate student and therefore research plays a very important role in my current life. My research interests mainly include cloud computing, decentralised social network, graph algorithms and machine learning. </p>

<p>My first research project is motivated from the simple and heuristic-based speculative execution approach under the MapReduce framework. Since the previous work lacks the theoretical understanding, I consider to tackle this issue from the perspective of system modelling. </p>

<p>To begin with, I build a very simple model to characterize the tradeoff between the job completion time and resource consumption under the speculative execution scheme. In the first optimization problem, I only consider one single job which consists of multiple tasks that can run in parallel on multiple machines. Moreover, the optimization objective is to minimize the resource consumption while satisfy the job’s SLA requirement. </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Matrix Calculation]]></title>
    <link href="http://xuhappy.github.io/blog/2015/01/16/matrix-calculation/"/>
    <updated>2015-01-16T21:34:18+08:00</updated>
    <id>http://xuhappy.github.io/blog/2015/01/16/matrix-calculation</id>
    <content type="html"><![CDATA[<h3 id="basic-notations">Basic notations:</h3>

<p>In the following descriptions, $x \in R^n $, $b \in R^n$ and $X \in
R^{n \times n}$, $A \in R^{n \times n}$ and $f (x) \in R, f (X) \in
R^{}$. To begin with, we first standardize the following basic notations:</p>

<ul>
  <li>
    <script type="math/tex; mode=display">\frac{\partial (b^T x)}{\partial x} = \frac{\partial (x^T
b)}{\partial x} = \nabla_x tr (b^{^T} x) = \nabla_x tr (b^{^{}} x^T)=b</script>
  </li>
  <li>
    <script type="math/tex; mode=display">\frac{\partial (tr (A^T X))}{\partial X} = \frac{\partial (tr (^{}
X^T A))}{\partial X} = \nabla_X tr (A^T X) = \nabla_X tr (A^{} X^T) = A</script>
  </li>
</ul>

<h3 id="another-way-to-illustrate-the-basic-notations">Another way to illustrate the basic notations:</h3>

<ul>
  <li>
    <script type="math/tex; mode=display">d [tr (f (x)] = tr(d(f(x)) = tr[(\nabla_x f (x))^T d x]</script>
  </li>
  <li>
    <script type="math/tex; mode=display">d [tr (f (X)] = tr(d(f(X)) = tr[(\nabla_X f (X))^T d X]</script>
  </li>
</ul>

<hr />

<h3 id="chain-rule">Chain rule:</h3>

<script type="math/tex; mode=display">d [tr (g (x) h (x)) = tr [d (g (x) \cdot h (x))]</script>

<script type="math/tex; mode=display"> tr [d (g (x) \cdot h (x))] = tr[(\nabla_x g (x))^T d x \cdot h (x) + g (x) \cdot (\nabla_x h (x))^T d x]</script>

<script type="math/tex; mode=display">d [tr (g (x) h (x)) =  tr [(h (x) (\nabla_x g (x))^T + g (x) \cdot (\nabla_x h (x))^T) d x]</script>

<p>An illustration of chain rule:</p>

<script type="math/tex; mode=display"> d (x^T A x) = tr [d (x^T A x)] = tr [d (x^T) A x + x^T A d x]</script>

<script type="math/tex; mode=display"> d (x^T A x) = tr [(d
   x)^T A x + x^T A d x] = tr [((A x)^T + x^T A) d x] </script>

<p>As such, </p>

<script type="math/tex; mode=display">\frac{\partial (x^T A x)}{\partial x} = ((A x)^T + x^T A)^T = (A + A^T) x</script>

<p>and </p>

<script type="math/tex; mode=display">d (tr [X A X]) = tr [(A X + X A) d X]</script>

<script type="math/tex; mode=display">\frac{\partial (tr [X
A X])}{\partial x} = (A X + X A)^T = X^T A^T + A^T X^T</script>

<hr />

<h3 id="derivative-of-determinant">Derivative of determinant:</h3>

<p>It holds that $A A^{\ast} = | A | I$ 
where $A^{ \ast }$ 
is the Adjugate matrix of $A$, hence, </p>

<script type="math/tex; mode=display">|A| = tr(A A^{\ast})</script>

<p>and </p>

<script type="math/tex; mode=display">\frac{\partial (|A|)}{\partial A} = \nabla_A [tr (A A^{\ast})] = (A^{\ast})^T</script>

<p>If $A$ is a non-singular matrix, the following equations hold:</p>

<script type="math/tex; mode=display">A^{\ast} = |A| A^{- 1}</script>

<script type="math/tex; mode=display">\frac{\partial(|A|)}{\partial A} = |A| (A^{- 1})^T</script>

<p>If A is also symmetric, </p>

<script type="math/tex; mode=display">A^{- 1} = {diag} (A^{-1})</script>

<p>It then immediately follows that: </p>

<script type="math/tex; mode=display">\frac{\partial (\ln | A |)}{\partial A} = (A^{-
1})^T</script>

<hr />

<h3 id="some-fundamental-tricks">Some fundamental tricks:</h3>

<ul>
  <li>
    <p>$A \in S^n$, $A = U \Sigma U^T$ where $U$ is an orthogonal matrix and $A = A^{1/2} A^{1/2}$ where $A^{1/2} = U \Sigma^{1/2} U^T$;</p>
  </li>
  <li>
    <p>If $| x | = 1$, then $(I + \lambda x x^T)^{- 1} = I -
\frac{\lambda}{1 + \lambda} x x^T$;</p>
  </li>
  <li>
    <p>$A \in S^{n}$, $\ln |I + A| = \sum_{i = 1}^n \ln (1 +
\lambda_i)$ where $\lambda_1, \lambda_2, \ldots, \lambda_n$ are the eigenvalues of $A$ and $\lambda_i &gt; - 1$. </p>
  </li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Research about MapReduce]]></title>
    <link href="http://xuhappy.github.io/blog/2015/01/16/research-about-mapreduce/"/>
    <updated>2015-01-16T20:06:35+08:00</updated>
    <id>http://xuhappy.github.io/blog/2015/01/16/research-about-mapreduce</id>
    <content type="html"><![CDATA[<p>In this blog, I mainly talk about three important research issues for MapReduce framework which are:</p>

<ul>
  <li><strong>Job scheduling for minimizing the total response time</strong></li>
  <li><strong>Data locality issue</strong></li>
  <li><strong>Speculative Execution</strong></li>
</ul>

<p>For each issue I will make two categories which are theoretical analysis based optimization and heuristic based algorithm  design. Hope you can get something useful from this summary.</p>

<hr />

<h2 id="some-new-papers-related-to-mapreduce-scheduling-and-task-cloning">Some new papers related to MapReduce scheduling and task cloning</h2>
<ul>
  <li><a href="https://www.eecs.berkeley.edu/~apanda/papers/kmn.pdf">The Power of Choice in Data-Aware Cluster Scheduling</a> This paper acutally studies two problems which are speculative execution and data locality. </li>
</ul>

<h2 id="job-scheduling">Job Scheduling</h2>

<h3 id="theoretical-analysis-based">Theoretical Analysis based:</h3>

<ul>
  <li>
    <p><a href="http://dl.acm.org/citation.cfm?id=1989493.1989540">B. Moseley, A. Dasgupta, R. Kumar, and T. Sarlos, “On scheduling in map-reduce and flow-shops,” in SPAA 2011.</a> This paper seeks to minimize the <em>flowtime</em> of jobs which is motivated from the two-stage flow shop problem. The tasks to machine mapping is unknown and needs to be determined such that the total <em>flowtime</em> is minimized. It gives the offline and online algorithm in both homogeneous and heterogenous environment. The analysis here is nontrivial and gives us a flavor how traditional SRPT algorithm can apply to the scheduling in MapReduce System. </p>
  </li>
  <li>
    <p><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5935152&amp;tag=1">Hyunseok Chang, Murali Kodialam, Ramana Rao Kompella, T. V. Lakshman, Myungjin Lee, Sarit Mukherjee, “Scheduling in MapReduce-like Systems for Fast completion time,” in Infocom 2011.</a> This paper seeks to optimize the total completion time of job running in the Mapreduce system. It decides the scheduling order of tasks on each processor. Moreover, this paper doesn’t consider the dependency between map task and reduce tasks. A linear program is formulated to relax the original problem. It then outlines a combinatorial approach to solve the LP problem.</p>
  </li>
  <li>
    <p><a href="http://www.cse.psu.edu/~fachen/pubs/infocom12mr.pdf">F. Chen, M. Kodialam, and T. Lakshman, “Joint scheduling of processing and shuffle phases in MapReduce systems,” in Infocom 2012.</a> 
Actually this paper is the follow-up work of the above paper. To be more practical, it considers the dependency between map and reduce tasks. Moreover, it jointly optimizes the job completion time by imposing the shuffle phase constraint. </p>
  </li>
  <li>
    <p><a href="http://dl.acm.org/citation.cfm?id=2254761">Jian Tan, Xiaoqiao Meng, Li Zhang, “Delay Tails in MapReduce Scheduling,” in SIGMETRICS 2012.</a> This paper focuses on the the starvation issue of small jobs under fair scheduler in MapReduce. The root reason of this problem is caused by launching the reduce tasks so early. To efficiently solve this issue and improve the job response performance, it presents a couple scheduler which assign the reduce tasks based on the progress of the map phase. The theoretical analysis here is very complicated. </p>
  </li>
  <li>
    <p><a href="http://newslab.ece.ohio-state.edu/research/resources/Analysis.pdf">Yousi Zheng, Ness Shroff, Prasun Sinha, “A New Analytical Technique for Designing Provably Efficient MapReduce Schedulers,” in Infocom 2013</a>. This paper is the first work to write down the formulation of mapreduce job scheduling including both map and reduce phases. There is no distinguish between map and reduce slots here. It uses adopts the knowledge in probability to prove that any work-conserving scheduler can achieves a constant efficiency ratio (not competitive ratio). Moreover, this paper presents the ASRPT algorithm which is inspired from the famous SRPT (Shortest remaining time first) algorithm and bound the competitive ratio to be 2.</p>
  </li>
  <li>
    <p><a href="http://users.cms.caltech.edu/~adamw/papers/preprint_mapreduce.pdf">M. Lin, L. Zhang, A. Wierman and J. Tan, “Joint optimization of overlapping phases in MapReduce,” in IFIP 2013.</a>. This is the first work to consider the overlapping of map phase and shuffle phase so far. A nice formulation is also written down here. Hover, even the offline case with batch arrival is shown to be NP-Complete. It then presents the MaxSRPT ans SpiltSRPT algorithm which are complementary to solve schedule the jobs.</p>
  </li>
</ul>

<h3 id="heuristic-based">Heuristic based:</h3>

<p>I haven’t read any papers which present heuristic-based algorithm to optimize the job completion time in MapReduce system. </p>

<hr />

<h2 id="data-locality">Data locality</h2>

<h3 id="theoretical-analysis-based-1">Theoretical Analysis based:</h3>

<ul>
  <li>
    <p><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6284711&amp;tag=1">Qiaomin Xie, Yi Lu, “Degree-guided map-reduce task assignment with data locality constraint,” in ISIT 2012.</a> This paper presents the peeling algorithm and adopts the mean-field equations to analyze the evolution (I cannot understand it well). However, it makes an impractical assumption which is that all the tasks with data locality on the machine can be finished in a time slot with probability <em>p</em> while the tasks without data locality can only be finished with probability <em>q</em> and <strong>p &gt; q</strong>. Also the theoretical here is very complex. </p>
  </li>
  <li>
    <p><a href="http://inlab.lab.asu.edu/Publications/WanZhuYin_13.pdf">W. Wang, K. Zhu, L. Ying, J. Tan, L. Zhang, “Map Task Scheduling in MapReduce with Data Locality: Throughput and Heavy-Traffic Optimality,” in Infocom 2013</a>. This paper makes the same assumption as the above paper presented in ISIT 2012. Its task scheduling algorithm includes two aspects which are <code>Routing</code> and <code>Scheduling</code>. The purpose of routing is to keep load balancing for all the machines. Also scheduling process makes a very simple decision based on the length of queues. It adopts the Lyapunov function to prove the stability of the algorithm. The analysis here is also nontrivial. </p>
  </li>
  <li>
    <p><a href="https://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;ved=0CDIQFjAB&amp;url=http%3A%2F%2Fresearcher.watson.ibm.com%2Fresearcher%2Ffiles%2Fus-tanji%2FreducePlace2013.pdf&amp;ei=Uh4rUq5mrMGJB-j5gNAJ&amp;usg=AFQjCNEGqVhyg8P3Vj5uOxx7QUrrei4ZQg">J. Tan, X. Meng, L. Zhang, “Improving ReduceTask Data Locality for Sequential MapReduce Jobs,” in Infocom 2013.</a> This is the first paper to analyze the data locality issue for reduce tasks with theoretical analysis. Actually, the model here is quite simple, the objective is only to minimize the data transfer cost in shuffle phase with proper reduce task placement. The data transfer cost gives a very good angle for analyzing data locality issue.  </p>
  </li>
</ul>

<h3 id="heuristic-based-1">Heuristic based:</h3>

<ul>
  <li>
    <p><a href="http://www.sigops.org/sosp/sosp09/papers/isard-sosp09.pdf">Michael Isard, Vijayan Prabhakaran, Jon Currey, Udi Wieder, Kunal Talwar and Andrew Goldberg, “Quincy: Fair Scheduling for Distributed Computing Clusters,” in SOSP 2009.</a> This is the first paper to address the data locality issue and fairness problem in MapReduce-like systems. It encodes the scheduling as a flow network. In this network, the edge weights encode the demands of data locality and fairness. This is a very novel and beautiful work. </p>
  </li>
  <li>
    <p><a href="http://dl.acm.org/citation.cfm?id=1755940">Matei Zaharia, Dhruba Borthakur, Joydeep Sen Sarma, Khaled Elmeleegy, Sunnyvale, Scott Shenker, Ion Stoica, “Delay scheduling: a simple technique for achieving locality and fairness in cluster scheduling,” in EuroSys 2010.</a> This paper is very famous among all the papers which address the data locality issue. It presents the delay scheduling algorithm which delay the job at the head of the queue a little bit if data locality is not achieved with the available slot. As described next, this algorithm has a obvious drawback: there exists a probability that some slots wont’e be launching any tasks and keeps idle all the way. This phenomenon waste the resource in the cluster and needs to be improved. </p>
  </li>
  <li>
    <p><a href="http://researcher.watson.ibm.com/researcher/files/us-tanji/coupling2013.pdf">J. Tan, X. Meng, L. Zhang, “Coupling Task Progress for MapReduce Resource-Aware Scheduling”, in Infocom 2013</a>. This paper mainly includes 3 pieces of work and for each work a heuristic-based algorithm I is proposed: 
1) Compute the mismatch between the map and reduce progress. The job gets the maximum mismatch value has the highest priority to assign a reduce task. 
 2) Waiting scheduling for reduce task. The main idea behind this algorithm is to assign a reduce task to an available slot who is close to the ‘centrality’ of the already generated intermediate date produced by the map phase. 
 3) Random peaking algorithm to assign the map task. This algorithm is motivated from the famous ‘delay scheduling’ algorithm may cause a bad resource utilization level. 
Actually, the heuristic algorithm for assigning map tasks is very complex to implement and doesn’t make a clear intuition.  However, the bad resource utilization for ‘Delay Scheduling’ is a very good observation.</p>
  </li>
</ul>

<hr />

<h2 id="speculative-execution">Speculative execution</h2>

<h3 id="heuristic-based-2">Heuristic based:</h3>

<ul>
  <li>
    <p><a href="http://research.google.com/archive/mapreduce-osdi04.pdf">J. Dean and S. Ghemawat, “MapReduce: Simplified Data Processing on Large Clusters”, in USENIX OSDI, 2004.</a>
 Actually, this paper is the technical report to the MapReduce system design of Google. It talks about the back up strategy with a short paragraph. The strategy is that  when a MapReduce operation is close to complete, it will randomly choose some remaining tasks and launch the duplicates of them on alternative machines. It concludes that this speculative execution scheme can decrease the job execution time by 44%.</p>
  </li>
  <li>
    <p><a href="http://bnrg.eecs.berkeley.edu/~randy/Courses/CS268.F08/papers/42_osdi_08.pdf">M. Zaharia, A. Konwinski, A. D. Joseph, R. Katz, and I. Stoica, “Improving mapreduce performance in heterogeneous environments,” in OSDI 2008.</a> In this paper, a new strategy named LATE is presented. LATE monitors the progress rate of the tasks and estimates their remaining time. Tasks with their progress rate below <em>slowTaskTherehold</em> are chosen as backup candidates and the one with the longest remaining time is given the highest priority when the number of backups in the cluster does not exceed <em>speculativeCap</em>.</p>
  </li>
  <li>
    <p><a href="http://www.computer.org/csdl/trans/tc/preprint/06419699-abs.html">G. Ananthanarayctions of Computer Networks 2013.</a> This paper proposes a Smart Speculative Execution algorithm. The main contributions it makes are twofold: i) Use exponentially weighted moving average to predict process speed and compute a task’s remaining time; ii) Determine which task to backup based on the load of a cluster using a cost-benefit model.</p>
  </li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">In our following research, we can consider to optimize 
</span><span class="line">the job scheduling in a heterogeneous environment where
</span><span class="line">the machines are not identical.</span></code></pre></td></tr></table></div></figure></notextile></div>
]]></content>
  </entry>
  
</feed>
